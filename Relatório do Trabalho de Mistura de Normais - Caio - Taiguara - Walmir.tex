\documentclass[12pt,reqno,a4paper,oneside]{article}

\usepackage{amsfonts, amsmath,
	        amssymb, amsthm}  % Fontes e símbolos matemáticos da AMS.
\usepackage{graphicx}         % Importação de gráficos.
\usepackage{subfig}
\usepackage[brazil]{babel}    % Idioma português (Brasil) como padrão.
\usepackage[utf8]{inputenc}   % Codificação UTF-8.
%\usepackage{natbib}           % Para citações.
%\usepackage{cite}             % Idem.

\usepackage{bm}          % Equações em negrito com qualidade superior.
\usepackage{booktabs}    % Edição de linhas de tabelas.
\usepackage{caption}     % Edição do texto dos títulos de tabelas.
\usepackage{color}       % Edição de cores.
\usepackage{indentfirst} % Indentará sempre o 1º parágrafo de cada seção do relatório.
%\usepackage{enumitem}   % Edição mais elaborada de listas
                         % Conflito com ambiente "enumerate" no beamer!.
%\usepackage{hyperref}   % Criação de "hyperlinks".
\usepackage{multirow}    % Para obter uma célula com uma coluna e várias linhas.
\usepackage{ragged2e}    % Opções de alinhamento do texto.

%\usepackage[FIGTOPCAP]{subfigure} % Acesso a figuras dentro de pastas.

\usepackage[top=2cm, bottom=2cm,
            left=2cm, right=2cm]{geometry} % Margens do texto.

\captionsetup{justification = centering} % Centraliza os títulos das tabelas.

% Símbolo matemático para independência entre variáveis aleatórias:

\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\begin{document}

\title{\huge Relatório do Trabalho de\\
	Estatística Bayesiana I}
\author{\\
	\\
	\huge Mistura de Normais\\
	\huge com Variância Contaminada\\
	\\
	\\
	\\
	\\
	\Large Caio Balieiro\\
	\Large Taiguara Melo Tupinambás\\
	\Large Walmir dos Reis Miranda Filho\\
	\\
	\\
	\\
	\\
	\Large Prof. Dani Gamerman\\
	\Large Profª Rosangela Helena Loschi\\
	\\
	\\
	\\
	\\
	\\
	Programa de Pós-Graduação em Estatística\\
	Instituto de Ciências Exatas\\
	Universidade Federal de Minas Gerais\\
	\\
	\\
	\\
	\\
	\\
	\\}
\date{Belo Horizonte, 2 de dezembro de 2019}
\maketitle

\newpage

\input{sec1introRT.tex}
\input{sec2qrRT.tex}
\input{sec3sirRT.tex}

\section{Monte Carlo via Cadeias de Markov (MCMC)}\label{mcmc}

Como alternativa ao método SIR dentro do conjunto das aproximações estocásticas, a integração via Monte Carlo em cadeias de Markov (MCMC) possui como ideia central construir cadeias de Markov, para cada variável aleatória de interesse, da qual seja possível gerar amostras e cuja distribuição limite seja estacionária e dada pela distribuição da própria variável (Migon \textit{et al.}, 2014)\cite{MiGaLou2014}. Uma cadeia de Markov é todo modelo estocástico que descreve uma sequência de eventos tal que a probabilidade de cada evento depende exclusivamente do estado atingido no evento anterior. Como grande vantagem em relação aos métodos de aproximação com quadraturas e ao SIR, o MCMC pode ser facilmente aplicado a problemas de alta dimensionalidade, inclusive quando se deseja gerar de vetores ou mesmo matrizes aleatórias.

Sejam $Y_1, \ldots, Y_p$ variáveis aleatórias com densidade conjunta $p(\bm{y}) = p(y_1, \ldots, y_p)$ definidas no espaço $\mathcal{Y} \subseteq \mathbb{R}^p$. Suponha que existe uma cadeia de Markov homogênea, irredutível e aperiódica com espaço de estados $\mathcal{Y}$ e distribuição estacionária $p(\bm{y})$. Seja $q(\bm{y}, \bm{z})$ o \textit{núcleo de transição da cadeia}, ou seja, $q(\bm{y}, \cdot)$ define uma distribuição condicional que governa as transições a partir do estado $\bm{y}$. Em outras palavras, a cadeia possui probabilidades de transição invariantes no tempo, na qual cada estado pode ser atingido a partir de qualquer outro em um número finito de iterações e sem haver estados absorventes. Assim, dado qualquer estado inicial, é possível gerar uma trajetória para a cadeia que convergirá para $p(\bm{y})$ para um número suficientemente grande de iterações. Construída a cadeia de Markov, é possível realizar uma simulação de Monte Carlo dos valores de $p$, por isso o nome MCMC.

Há vários algoritmos na literatura que permitem construir uma cadeia de Markov com distribuição limite estacionária. Neste trabalho, será usado o algoritmo de Metropolis-Hastings (Metropolis \textit{et al.}, 1953\cite{Metrop1953}; Hastings, 1970\cite{Hastin1970}), aqui abreviado por MH. Assim como o método SIR, o algoritmo MH também é baseado no uso de uma distribuição auxiliar ou proposta, aqui denotada por $q(\bm{y}, \bm{z})$. Assumindo-se que na interação $j$, $j = 1, \ldots, k$ a cadeia está no estado $\bm{y}^{(j)}$, a posição da mesma na iteração $j + 1$, denotada por $\bm{y}^{(j + 1)}$, será dada após:
\begin{itemize}
	\item Propor uma transição ou movimento para $\bm{y}^*$, onde $\bm{y}^*$ é gerada de $q(\bm{y}^{(j)}, \cdot)$, a distribuição proposta;
	\item Aceitar a transição proposta com probabilidade
	\begin{equation}\label{eq:mh_tranprob}
	\rho(\bm{y}^{(j)}, \bm{y}^*) = \min\left(1, \dfrac{p(\bm{y}^*) / q(\bm{y}^{(j)}, \bm{y}^*)}{p(\bm{y}^{(j)}) / q(\bm{y}^*, \bm{y}^{(j)})}\right)
	\end{equation}
	e neste caso atribuir $\bm{y}^{(j + 1)} = \bm{y}^*$ ou rejeitar a transição proposta e atribuir $\bm{y}^{(j + 1)} = \bm{y}^{(j)}$, com probabilidade $1 - \rho(\bm{y}^{(j)}, \bm{y}^*)$.
\end{itemize}

Para decidir sobre a aceitação ou não de $\bm{y}^*$ quando amostrada a cada passo $j$, gere uma amostra $u_1, \ldots, u_k$, onde $k$ é o total de iterações prefixadas, da distribuição uniforme padrão $U(0,1)$, independentemente de $\bm{y}^*$. Se a probabilidade de aceitação $\rho(\bm{y}^{(j)}, \bm{y}^*)$ for maior do que ou igual a $u_j$, então a transição proposta é aceita. Do contrário, ela é rejeitada.

Note ainda que se a distribuição proposta $q$ é simétrica, pode-se reescrever $\rho(\bm{y}^{(j)}, \bm{y}^*)$ em \eqref{eq:mh_tranprob} como o mínimo entre o valor unitário e a razão $p(\bm{y}^*)/p(\bm{y^{(j)}})$, simplificando o cálculo desta probabilidade. Após gerar uma amostra muito grande da variável de interesse, a inferência sobre a mesma pode ser feita assim como no SIR e em qualquer método de Monte Carlo, aproximando (por exemplo) a média populacional pela média amostral dos valores da cadeia.

\begin{figure}[t]%
	\centering
	\subfloat[Histograma de $\mu$]{{
			\label{fig:mu_mh_500}
			\includegraphics[scale=0.4]{figuras/mu_mh_500.pdf}}}%
	\qquad
	\subfloat[Histograma de $\sigma^2$]{{
			\label{fig:s2_mh_500}
			\includegraphics[scale=0.4]{figuras/s2_mh_500.pdf}}}%
	\subfloat[Histograma de $\nu$]{{
			\label{fig:nu_mh_500}
			\includegraphics[scale=0.4]{figuras/nu_mh_500.pdf}}}%
	\caption{Histograma das densidades \textit{a posteriori} marginais pelo método MCMC--MH, $k = 500$}%
\end{figure}

\begin{figure}[t]%
	\centering
	\subfloat[Histograma de $\mu$]{{
			\label{fig:mu_mh_5000}
			\includegraphics[scale=0.4]{figuras/mu_mh_5000.pdf}}}%
	\qquad
	\subfloat[Histograma de $\sigma^2$]{{
			\label{fig:s2_mh_5000}
			\includegraphics[scale=0.4]{figuras/s2_mh_5000.pdf}}}%
	\subfloat[Histograma de $\nu$]{{
			\label{fig:nu_mh_5000}
			\includegraphics[scale=0.4]{figuras/nu_mh_5000.pdf}}}%
	\caption{Histograma das densidades \textit{a posteriori} marginais pela método MCMC--MH, $k = 5000$}%
\end{figure}

\begin{figure}[t]%
	\centering
	\subfloat[Histograma de $\mu$]{{
			\label{fig:mu_mh_50000}
			\includegraphics[scale=0.4]{figuras/mu_mh_50000.pdf}}}%
	\qquad
	\subfloat[Histograma de $\sigma^2$]{{
			\label{fig:s2_mh_50000}
			\includegraphics[scale=0.4]{figuras/s2_mh_50000.pdf}}}%
	\subfloat[Histograma de $\nu$]{{
			\label{fig:nu_mh_50000}
			\includegraphics[scale=0.4]{figuras/nu_mh_50000.pdf}}}%
	\caption{Histograma das densidades \textit{a posteriori} marginais pela método MCMC--MH, $k = 50000$}%
\end{figure}

No presente trabalho, deseja-se amostrar os parâmetros $\mu, \sigma^2$ e $\nu$ da distribuição \textit{a posteriori} $p(\mu, \sigma^2, \nu | \bm{x})$ do modelo de mistura finita de normais com variância contaminada. O uso do algoritmo MH para cada uma das 3 cadeias correspondentes se justifica pelo fato de que: (i) em nenhum dos parâmetros é possível obter analiticamente a respectiva distribuição condicional completa, de modo que será necessário amostrar da distribuição \textit{a posteriori} conjunta, e (ii) como a distribuição \textit{a posteriori} conjunta é escrita como o produto de uma constante de proporcionalidade vezes um núcleo que depende dos parâmetros, o algoritmo MH continua válido quando se trabalha com o núcleo do lado direito de \eqref{eq:dist_post} no lugar de $p(\mu, \sigma^2, \nu | \bm{x})$. Novamente, para avaliar este núcleo, toma-se a soma do logaritmo de todas as quantidades que o compõem quando multiplicadas entre si. Assim como feito no método SIR, para a distribuição proposta $q(\cdot)$ também será escolhida uma normal trivariada $N_3(\bm{\mu}, \bm{\Sigma})$ e a mesma reparametrização para $(\mu, \sigma^2, \nu)$ será reutilizada de modo a garantir que a distribuição proposta gere adequadamente de $p(\mu, \sigma^2, \nu | \bm{x})$ pelo algoritmo MH, inclusive com os mesmos valores do vetor de médias $\bm{\mu} = (\mu, \log(\sigma^2), \log[\nu/(1-\nu)]) = (11, \log(0.64), \log[0.2/0.8])$ e da matriz de covariância $\bm{\Sigma} = \textrm{diag}\{0.0022, 0.0065, 0.0203\}$.

Para aplicar o método MCMC com algoritmo MH de modo a obter amostras \textit{a posteriori} de $\mu$, $\sigma^2$ e $\nu$, foram considerados 3 cenários: $k = \{500, 5000, 50000\}$, os mesmos do método SIR. Como no MCMC é necessário definir um estado inicial da cadeia, em geral com densidade conjunta muito baixa, escolheu-se o ponto $x_0 = (10.86, log(0.50), log(0.14/0.86))$ para inicialização do algoritmo, com logaritmo do núcleo da \textit{posteriori} avaliado em $1.760 \times 10^{-235}$, ainda numericamente não nulo pelo \textit{R}. Evidentemente, as primeiras amostras geradas não terão distribuição próxima à da limite, razão pela qual será feito um descarte (em inglês, \textit{burn-in}) de 20\% a partir dos valores iniciais. Logo, ao final se terão $400, 4000$ e $40000$ valores gerados nos cenários correspondentes. Como o vetor de valores uniformes para comparação das probabilidades de aceitação a cada passo não dependem dos valores gerados de $q$, em todos os cenários eles foram construídos fora da rotina iterativa do algoritmo MH. No final, as taxas de aceitação das amostras propostas ficaram em $45.2\%$; $40.42\%$ e $41.11\%$ para os cenários em ordem crescente de tamanho da cadeia. Nas figuras \ref{fig:mu_mh_500} a \ref{fig:nu_mh_500}; \ref{fig:mu_mh_5000} a \ref{fig:nu_mh_5000} e \ref{fig:mu_mh_50000} a \ref{fig:nu_mh_50000} são apresentados os histogramas para as amostras \textit{a posteriori} de cada parâmetro por cenário.

Para $k=500$, a aproximação da densidade não é boa para nenhum dos 3 parâmetros com as $400$ amostras restantes: há pontos extremos distantes do que seria a massa principal, especialmente para $\nu$. Além disso, há várias modas locais, indicando uma curva que está longe de ser suave. Entretanto, é possível dizer já neste cenário como é o comportamento de cada parâmetro com relação à locação, assim como no método SIR.

Para $k=5000$, a aproximação é bem mais suave e há menos \textit{outliers} quando consideradas as $4000$ amostras restantes. O número de modas locais também é reduzido em comparação ao cenário anterior, embora esta quantidade ainda seja moderada para $\sigma^2$ e alta para $\nu$. Além do comportamento com relação à locação, é possível dizer também como é o comportamento com relação à dispersão da densidade \textit{a posteriori} marginal de cada parâmetro, mas não com relação à assimetria, ao contrário do método SIR.

Para $k=50000$, os gráficos confirmam a tendência apresentada pelo cenário anterior, mas com uma suavização ainda melhor, embora não tão boa quanto no algoritmo SIR para a mesma quantidade de amostras \textit{a posteriori} geradas, mesmo com o \textit{burn-in} de 20\% das amostras. Isso se pelo fato de que o começo da cadeia é fortemente influenciada pela escolha do valor inicial mesmo com o \textit{burn-in}, mas essa influência vai diminuindo ao longo dos passos do algoritmo. Todas as distribuições são praticamente unimodais (há algumas modas locais quase imperceptíveis nos gráficos de $\mu$ e $\sigma^2$) e praticamente não há mais \textit{outliers}. Agora, é possível identificar uma leve assimetria à direita nos gráficos de $\sigma^2$ e $\nu$.

Obtidas as amostras \textit{a posteriori} marginais de cada parâmetro, para aproximar as estatísticas de média, variância, assimetria e curtose novamente serão tomadas as respectivas estimativas amostrais. Na Tabela \ref{tab3}, são apresentados os valores aproximados para tais estatísticas em cada um dos três cenários.

\begin{table}[htb]
	\caption{Estatísticas \textit{a posteriori} para $(\mu, \sigma^2, \nu)$ pelo método MCMC--MH}
	\label{tab3}
	\centering
	\begin{tabular}{cccccc}
		\toprule
		Cenário & Parâmetro & Média & Variância & Assimetria & Curtose \\
		\midrule
		$k = 500$ & $\mu$ & 10.9672 & 0.0018 & -0.3768 & 3.2193 \\
		& $\sigma^2$ & 0.6226 & 0.0020 & -0.0684 & 2.6117 \\
		& $\nu$      & 0.1957 & 0.0006 & -0.3313 & 3.2991 \\
		\midrule
		$k = 5000$ & $\mu$ & 10.9818 & 0.0019 & -0.2466 & 2.9894 \\
		& $\sigma^2$ & 0.6197 & 0.0023 & 0.2372 & 3.0057 \\
		& $\nu$      & 0.1977 & 0.0005 & 0.3878 & 3.2038 \\
		\midrule
		$k = 50000$ & $\mu$ & 10.9852 & 0.0017 & -0.0272 & 2.9451 \\
		& $\sigma^2$ & 0.6188 & 0.0021 & 0.2472 & 3.1088 \\
		& $\nu$      & 0.1978 & 0.0005 & 0.1331 & 2.8971 \\
		\bottomrule
	\end{tabular}
\end{table}

Pela Tabela \ref{tab3}, pode-se concluir que tanto a média quanto a variância \textit{a posteriori} variaram muito pouco nos três cenários, independente do parâmetro considerado. Logo, amostras \textit{a posteriori} de tamanho moderado são o suficiente para se obter uma boa aproximação destas duas estatísticas. Por outro lado, o mesmo não pode ser dito para a assimetria e curtose \textit{a posteriori}, para as quais há grandes mudanças inclusive na primeira casa decimal, mesmo de $k=5000$ para $k=50000$, assim como no método SIR (Tabela \ref{tab2}). Apesar deste fato, no método MCMC com passos MH também se pode dizer que houve uma estabilidade nas aproximações destas duas últimas estatísticas na medida em que $k$ crescia, embora com convergência mais lenta do que no método SIR.

Com relação aos valores em si, as médias \textit{a posteriori} para os 3 parâmetros ficaram muito próximas dos respectivos valores do modelo para a distribuição amostral, embora não tanto quanto no método SIR. Para as variâncias \textit{a posteriori}, todas elas foram bem pequenas, em especial para $\nu$, assim como na quadratura e no SIR. Com relação à assimetria \textit{a posteriori}, esta foi praticamente nula para $\mu$ e positiva, mas fraca, para $\sigma^2$ e $\nu$ (um pouco mais forte para a primeira, assim como na quadratura e diferente do método SIR). Por fim, as aproximações para a curtose \textit{a posteriori} foram todas bem próximas de 3 (em geral, acima deste valor), com o parâmetro $\mu$ mais próximo desse valor e $\sigma^2$ o mais afastado, a mesma conclusão na quadratura.

Em anexo, são publicados os gráficos de convergência e de autocorrelação das 9 cadeias geradas (3 para cada cenário) ...

%Texto descrevendo o método e como foi feita neste trabalho a geração de variáveis aleatórias da distribuição de interesse usando este mesmo método, bem como o amostrador selecionado (possivelmente Gibbs ou outro que for de melhor uso para todo o grupo).

\section{Considerações Finais}\label{consfin}

%Texto descrevendo de forma resumida o que foi feito nas seções anteriores, bem como os resultados dos métodos quanto à convergência, à medida que se aumenta o tamanho amostral m da distribuição \textit{a poseteriori}.

\bibliographystyle{unsrt}
\bibliography{ref}

\newpage

\section*{Apêndice: Código R para os Resultados Apresentados}

\subsection*{Introdução}

\begin{verbatim}
# Tamanho amostral "n"; parâmetros "mu", "s2" (sigma^2) e "nu" do modelo para
# a distribuição amostral e hiperparâmetros m, V, a e d das distribuições "a
# priori" (somente para "mu" e "s2"):

n = 500; mu = 11; s2 = 0.64; nu = 0.2; m = 11; V = 1; a = 7; d = 4

# Geração hierárquica da amostra de tamanho "n" do modelo:

U_pdf = function(n, nu) {
	u = sample(c(100, 1), prob=c(nu, 1-nu), size=n, replace=T)
}

set.seed(122019)

u = U_pdf(n, nu)
sam = rnorm(n, mu, sqrt(s2*u))

# Histograma da amostra gerada do modelo (figura 1):

par(mar = c(5,5,3,2))
hist(sam, breaks=100, prob=T, main="", xlim = c(-10, 40), ylim = c(0, 0.5),
	 xlab = "Amostra do modelo", ylab = "Densidade empírica")

# Cálculo do logaritmo do núcleo da distribuição "a posteriori" original:

logA = function(X, mu, sigma2, nu) {
	n = length(X)
	k1 = (nu/10)*exp(-(X-mu)^2/(2*100*sigma2))
	k2 = (1-nu)*exp(-(X-mu)^2/(2*sigma2))
	k = log(k1 + k2)
	return(k)
}

logh = function(n, mu, sigma2, nu, m, V, a, d) {
	k1 = -((n + 1)/2 + a + 1)*log(sigma2)
	k2 = -((mu - m)^2/(2*V) + d)/sigma2
	k = k1 + k2
	return(k)
}

logkpost = function(X, mu, sigma2, nu, m, V, a, d) {
	n = length(X)
	lA = logA(X, mu, sigma2, nu)
	lh = logh(n, mu, sigma2, nu, m, V, a, d)
	lkp = sum(lA) + lh
	return(lkp)
}

# Gráficos dos intervalos de massa probabilística para cada parâmetro do
# núcleo da distribuição "a posteriori", fixados os demais, após algumas
# tentativas anteriores para redução dos limites do gráfico e melhor vi-
# sualização da curva (figuras 2a a 2c):

mu_sup=seq(10.8, 11.2, 0.001); t_mu=length(mu_sup); kp_mu=numeric(t_mu)

for(i in 1:t_mu) {
	kp_mu[i] = exp(logkpost(X=sam,
	mu=mu_sup[i], sigma2=s2, nu=nu, m=m, V=V, a=a, d=d))
	}
plot(mu_sup, kp_mu, type="l", main="",
     xlab=expression(paste(mu)), ylab="")

s2_sup=seq(0.4, 0.8, 0.001); t_s2=length(s2_sup); kp_s2=numeric(t_s2)

for(i in 1:t_s2) {
	kp_s2[i] = exp(logkpost(X=sam,
	mu=mu, sigma2=s2_sup[i], nu=nu, m=m, V=V, a=a, d=d))
	}
plot(s2_sup, kp_s2, type="l", main="",
	 xlab=expression(paste(sigma^2)), ylab="")

nu_sup=seq(0.1, 0.3, 0.001); t_nu=length(nu_sup); kp_nu=numeric(t_nu)

for(i in 1:t_nu) {
	kp_nu[i] = exp(logkpost(X=sam,
	mu=mu, sigma2=s2, nu=nu_sup[i], m=m, V=V, a=a, d=d))
	}
plot(nu_sup, kp_nu, type="l", main="",
     xlab=expression(paste(nu)), ylab="")
\end{verbatim}

\subsection*{O Método da Quadratura de Riemann}

\begin{verbatim}

# Cenários considerados, variando no número de subintervalos utilizados:

L1 = 15; L2 = 50; L3 = 100

# Tamanhos dos subintervalos, dada a combinação de cenário e parâmetro, e
# grades definidas pelos mesmos:

mu_step1 = (11.13 - 10.85)/L1; mu_gr1 = seq(10.85, 11.13, mu_step1)
mu_step2 = (11.13 - 10.85)/L2; mu_gr2 = seq(10.85, 11.13, mu_step2)
mu_step3 = (11.13 - 10.85)/L3; mu_gr3 = seq(10.85, 11.13, mu_step3)

s2_step1 = (0.78 - 0.48)/L1; s2_gr1 = seq(0.48, 0.78, s2_step1)
s2_step2 = (0.78 - 0.48)/L2; s2_gr2 = seq(0.48, 0.78, s2_step2)
s2_step3 = (0.78 - 0.48)/L3; s2_gr3 = seq(0.48, 0.78, s2_step3)

nu_step1 = (0.26 - 0.13)/L1; nu_gr1 = seq(0.13, 0.26, nu_step1)
nu_step2 = (0.26 - 0.13)/L2; nu_gr2 = seq(0.13, 0.26, nu_step2)
nu_step3 = (0.26 - 0.13)/L3; nu_gr3 = seq(0.13, 0.26, nu_step3)

# Combinando as três grades unidimensionais em cada cenário, todas com
# número igual de subintervalos e de pontos (nos limites dos interval-
# os) para integração numérica:

grid_tri1 = cbind(mu_gr1, s2_gr1, nu_gr1); l1 = nrow(grid_tri1)
grid_tri2 = cbind(mu_gr2, s2_gr2, nu_gr2); l2 = nrow(grid_tri2)
grid_tri3 = cbind(mu_gr3, s2_gr3, nu_gr3); l3 = nrow(grid_tri3)

# Produto triplo dos tamanhos dos subintervalos em cada cenário:

prod_step1 = mu_step1*s2_step1*nu_step1
prod_step2 = mu_step2*s2_step2*nu_step2
prod_step3 = mu_step3*s2_step3*nu_step3

# Cálculo do inverso da constante de proporcionalidade em cada cenário:

cprop = function(l, X, mgr, s2gr, ngr, prst, m, V, a, d) {
	c = 0
	for (i in 1:l) {
		for (j in 1:l) {
			for (k in 1:l) {
				aux = logkpost(X, mgr[i], s2gr[j], ngr[k], m, V, a, d)
				c = c + exp(aux)*prst
			}
		}
	}
	return(c)
}

c1 = cprop(l=l1, X=sam, mgr=mu_gr1, s2gr=s2_gr1, ngr=nu_gr1,
		   prst=prod_step1, m=m, V=V, a=a, d=d)
c2 = cprop(l=l2, X=sam, mgr=mu_gr2, s2gr=s2_gr2, ngr=nu_gr2,
		   prst=prod_step2, m=m, V=V, a=a, d=d)
c3 = cprop(l=l3, X=sam, mgr=mu_gr3, s2gr=s2_gr3, ngr=nu_gr3,
		   prst=prod_step3, m=m, V=V, a=a, d=d)
c1; c2; c3

# Cálculo das densidades "a posteriori" marginais em cada cenário, com
# uso do produto duplo dos tamanhos dos subintervalos em cada cenário:

pr1_dup12 = mu_step1*s2_step1
pr1_dup13 = mu_step1*nu_step1
pr1_dup23 = s2_step1*nu_step1
\end{verbatim}

\newpage

\begin{verbatim}
pr2_dup12 = mu_step2*s2_step2
pr2_dup13 = mu_step2*nu_step2
pr2_dup23 = s2_step2*nu_step2

pr3_dup12 = mu_step3*s2_step3
pr3_dup13 = mu_step3*nu_step3
pr3_dup23 = s2_step3*nu_step3

postmu_quarie = function(l, X, mgr, s2gr, ngr, prst, m, V, a, d, c) {
	postmu = numeric(l)
	for (i in 1:l) {
		postconj = 0
		for (j in 1:l) {
			for(k in 1:l) {
				aux = logkpost(X, mgr[i], s2gr[j], ngr[k], m, V, a, d)
				postconj = postconj + exp(aux)*prst
			}
		}
		postmu[i] = postconj/c
	}
	return(postmu)
}

pmq1 = postmu_quarie(l=l1, X=sam, mgr=mu_gr1, s2gr=s2_gr1, ngr=nu_gr1,
					 prst=pr1_dup23, m=m, V=V, a=a, d=d, c=c1)
pmq2 = postmu_quarie(l=l2, X=sam, mgr=mu_gr2, s2gr=s2_gr2, ngr=nu_gr2,
					 prst=pr2_dup23, m=m, V=V, a=a, d=d, c=c2)
pmq3 = postmu_quarie(l=l3, X=sam, mgr=mu_gr3, s2gr=s2_gr3, ngr=nu_gr3,
					 prst=pr3_dup23, m=m, V=V, a=a, d=d, c=c3)

plot(mu_gr1,pmq1,type="l",main="",xlab=expression(paste(mu)),ylab="")
plot(mu_gr2,pmq2,type="l",main="",xlab=expression(paste(mu)),ylab="")
plot(mu_gr3,pmq3,type="l",main="",xlab=expression(paste(mu)),ylab="")

posts2_quarie = function(l, X, mgr, s2gr, ngr, prst, m, V, a, d, c) {
	posts2 = numeric(l)
	for (i in 1:l) {
		postconj = 0
		for (j in 1:l) {
			for(k in 1:l) {
				aux = logkpost(X, mgr[j], s2gr[i], ngr[k], m, V, a, d)
				postconj = postconj + exp(aux)*prst
			}
		}
		posts2[i] = postconj/c
	}
	return(posts2)
}

\end{verbatim}

\newpage

\begin{verbatim}
psq1 = posts2_quarie(l=l1, X=sam, mgr=mu_gr1, s2gr=s2_gr1, ngr=nu_gr1,
					 prst=pr1_dup13, m=m, V=V, a=a, d=d, c=c1)
psq2 = posts2_quarie(l=l2, X=sam, mgr=mu_gr2, s2gr=s2_gr2, ngr=nu_gr2,
					 prst=pr2_dup13, m=m, V=V, a=a, d=d, c=c2)
psq3 = posts2_quarie(l=l3, X=sam, mgr=mu_gr3, s2gr=s2_gr3, ngr=nu_gr3,
					 prst=pr3_dup13, m=m, V=V, a=a, d=d, c=c3)

plot(s2_gr1, psq1, type="l", main="", xlab=expression(paste(sigma^2)),
	 ylab="")
plot(s2_gr2, psq2, type="l", main="", xlab=expression(paste(sigma^2)),
	 ylab="")
plot(s2_gr3, psq3, type="l", main="", xlab=expression(paste(sigma^2)), 
	 ylab="")

postnu_quarie = function(l, X, mgr, s2gr, ngr, prst, m, V, a, d, c) {
	postnu = numeric(l)
	for (i in 1:l) {
		postconj = 0
		for (j in 1:l) {
			for(k in 1:l) {
				aux = logkpost(X, mgr[j], s2gr[k], ngr[i], m, V, a, d)
				postconj = postconj + exp(aux)*prst
			}
		}
		postnu[i] = postconj/c
	}
	return(postnu)
}

pnq1 = postnu_quarie(l=l1, X=sam, mgr=mu_gr1, s2gr=s2_gr1, ngr=nu_gr1,
					 prst=pr1_dup12, m=m, V=V, a=a, d=d, c=c1)
pnq2 = postnu_quarie(l=l2, X=sam, mgr=mu_gr2, s2gr=s2_gr2, ngr=nu_gr2,
					 prst=pr2_dup12, m=m, V=V, a=a, d=d, c=c2)
pnq3 = postnu_quarie(l=l3, X=sam, mgr=mu_gr3, s2gr=s2_gr3, ngr=nu_gr3,
					 prst=pr3_dup12, m=m, V=V, a=a, d=d, c=c3)

plot(nu_gr1,pnq1,type="l",main="",xlab=expression(paste(nu)),ylab="")
plot(nu_gr2,pnq2,type="l",main="",xlab=expression(paste(nu)),ylab="")
plot(nu_gr3,pnq3,type="l",main="",xlab=expression(paste(nu)),ylab="")
\end{verbatim}

\newpage

\begin{verbatim}
# Cálculo da média, variância, assimetria e curtose "a posteriori" para
# cada parâmetro:

stat_post = function(gr, marg, prst) {
	media = 0
	var = 0
	assim = 0
	cur = 0
	l = length(gr)
	
	aux1 = sum(gr*marg*prst)     # Aproxima 1º momento.
	aux2 = sum((gr^2)*marg*prst) # Aproxima 2º momento.
	aux3 = sum((gr^3)*marg*prst) # Aproxima 3º momento.
	aux4 = sum((gr^4)*marg*prst) # Aproxima 4º momento.
	
	media = aux1
	var = aux2 - (media)^2
	assim = (aux3 - 3*media*var - media^3)/(var^(3/2))
	cur = (aux4 - 4*media*aux3 + 6*media^2*aux2 - 3*(media^4))/(var^2)
	return(list(media, var, assim, cur))
}

stat_post(mu_gr1, pmq1, mu_step1)
stat_post(mu_gr2, pmq2, mu_step2)
stat_post(mu_gr3, pmq3, mu_step3)

stat_post(s2_gr1, psq1, s2_step1)
stat_post(s2_gr2, psq2, s2_step2)
stat_post(s2_gr3, psq3, s2_step3)

stat_post(nu_gr1, pnq1, nu_step1)
stat_post(nu_gr2, pnq2, nu_step2)
stat_post(nu_gr3, pnq3, nu_step3)

\end{verbatim}

\subsection*{O Método da Reamostragem Por Importância Sequencial (SIR)}

\begin{verbatim}
# Cálculo do logaritmo do núcleo da distribuição "a posteriori" reparametrizada:

logr = function(sigma2, nu) {
k = log(sigma2) + 3*log(nu) - log(1-nu)
return(k)
}
\end{verbatim}

\newpage

\begin{verbatim}
logkpost_re = function(X, mu, sigma2, nu, m, V, a, d) {
n = length(X)
lA = logA(X, mu, sigma2, nu)
lh = logh(n, mu, sigma2, nu, m, V, a, d)
lr = logr(sigma2, nu)
lkrp = sum(lA) + lh + lr
return(lkrp)
}

logkpost_re(sam, mu, s2, nu, m, V, a, d)

# Cenários de tamanhos para amostragem da "posteriori" via SIR:

am1 = 500; am2 = 5000; am3 = 50000

# Valores reparametrizados da diagonal principal da matriz de covariâncias
# da distribuição normal trivariada:

((11.13-10.85)/6)^2
((log(0.78)-log(0.48))/6)^2
((log(0.26/0.74)-log(0.13/0.87))/6)^2

# Geração de amostras da distribuição normal trivariada:

library(mvtnorm)

set.seed(122019)
r1_q = rmvnorm(am1, mean=c(11, log(0.64), log(0.2/0.8)),
			   sigma=diag(c(0.0022, 0.0065, 0.0203)))
set.seed(122019)
r2_q = rmvnorm(am2, mean=c(11, log(0.64), log(0.2/0.8)),
			   sigma=diag(c(0.0022, 0.0065, 0.0203)))
set.seed(122019)
r3_q = rmvnorm(am3, mean=c(11, log(0.64), log(0.2/0.8)),
			   sigma=diag(c(0.0022, 0.0065, 0.0203)))

# Cálculo da densidade para cada ponto das amostras geradas:

d1_q = dmvnorm(r1_q, mean=c(11, log(0.64), log(0.2/0.8)), sigma=diag(3))
d2_q = dmvnorm(r2_q, mean=c(11, log(0.64), log(0.2/0.8)), sigma=diag(3))
d3_q = dmvnorm(r3_q, mean=c(11, log(0.64), log(0.2/0.8)), sigma=diag(3))
\end{verbatim}

\newpage

\begin{verbatim}
# Cálculo dos pesos de reamostragem:

weiSIR = function(X, r_q, m, V, a, d, d_q) {
	k = nrow(r_q)
	aux1 = aux2 = w = numeric(k)
	for(i in 1:k) {
		aux1[i] = logkpost_re(X=X, mu=r_q[i,1], sigma2=exp(r_q[i,2]),
							  nu=1/(1+exp(-r_q[i,3])), m=m, V=V, a=a,
							  d=d)
	}
	aux2 = aux1/d_q
	w = aux2/sum(aux2)
	return(w)
}

w1 = weiSIR(sam, r1_q, m, V, a, d, d1_q); sum(w1)
w2 = weiSIR(sam, r2_q, m, V, a, d, d2_q); sum(w2)
w3 = weiSIR(sam, r3_q, m, V, a, d, d3_q); sum(w3)

# Geração das amostras da densidade aproximada pelo método SIR com os pesos
# calculados acima:

set.seed(122019)
am1_mu_p = sample(x=r1_q[,1], size=am1, replace=T, prob=w1)
set.seed(122019)
am1_s2_p = sample(x=exp(r1_q[,2]), size=am1, replace=T, prob=w1)
set.seed(122019)
am1_nu_p = sample(x=1/(1+exp(-r1_q[,3])), size=am1, replace=T, prob=w1)

set.seed(122019)
am2_mu_p = sample(x=r2_q[,1], size=am2, replace=T, prob=w2)
set.seed(122019)
am2_s2_p = sample(x=exp(r2_q[,2]), size=am2, replace=T, prob=w2)
set.seed(122019)
am2_nu_p = sample(x=1/(1+exp(-r2_q[,3])), size=am2, replace=T, prob=w2)

set.seed(122019)
am3_mu_p = sample(x=r3_q[,1], size=am3, replace=T, prob=w3)
set.seed(122019)
am3_s2_p = sample(x=exp(r3_q[,2]), size=am3, replace=T, prob=w3)
set.seed(122019)
am3_nu_p = sample(x=1/(1+exp(-r3_q[,3])), size=am3, replace=T, prob=w3)
\end{verbatim}

\newpage

\begin{verbatim}
# Média; variância; assimetria; curtose e histogramas das amostras geradas:

library(moments)

st_sam_post = function(Xpar) {
	medi = mean(Xpar)
	vari = var(Xpar)
	assi = skewness(Xpar)
	curt = kurtosis(Xpar)
	return(list(media=medi, variancia=vari, assimetria=assi, curtose=curt))
}

st_sam_post(am1_mu_p)
hist(am1_mu_p, breaks=50, prob=T, main="",
	 xlab=expression(paste(mu)), ylab="")
st_sam_post(am2_mu_p)
hist(am2_mu_p, breaks=50, prob=T, main="",
	 xlab=expression(paste(mu)), ylab="")
st_sam_post(am3_mu_p)
hist(am3_mu_p, breaks=50, prob=T, main="",
	 xlab=expression(paste(mu)), ylab="")

st_sam_post(am1_s2_p)
hist(am1_s2_p, breaks=50, prob=T, main="",
	 xlab=expression(paste(sigma^2)), ylab="")
st_sam_post(am2_s2_p)
hist(am2_s2_p, breaks=50, prob=T, main="",
	 xlab=expression(paste(sigma^2)), ylab="")
st_sam_post(am3_s2_p)
hist(am3_s2_p, breaks=50, prob=T, main="",
	 xlab=expression(paste(sigma^2)), ylab="")

st_sam_post(am1_nu_p)
hist(am1_nu_p, breaks=50, prob=T, main="",
	 xlab=expression(paste(nu)), ylab="")
st_sam_post(am2_nu_p)
hist(am2_nu_p, breaks=50, prob=T, main="",
	 xlab=expression(paste(nu)), ylab="")
st_sam_post(am3_nu_p)
hist(am3_nu_p, breaks=50, prob=T, main="",
	 xlab=expression(paste(nu)), ylab="")
\end{verbatim}

\newpage

\subsection*{O Método de Monte Carlo via Cadeias de Markov (MCMC)}

\begin{verbatim}
# Cenários de tamanhos para amostragem da "posteriori" via MCMC com passos MH:

am1 = 500; am2 = 5000; am3 = 50000

# Geração de 3 amostras de uma uniforme padrão para o critério de aceitação no
# algoritmo de Metropolis-Hastings:

set.seed(122019); up1 = runif(am1)
set.seed(122019); up2 = runif(am2)
set.seed(122019); up3 = runif(am3)

# Ponto inicial da cadeia:

x = c(10.86, log(0.50), log(0.14/0.86))

# Vetores para receber amostras "a posteriori" da distribuição desejada:

am1_mu_p = am1_s2_p = am1_nu_p = numeric(am1)
am2_mu_p = am2_s2_p = am2_nu_p = numeric(am2)
am3_mu_p = am3_s2_p = am3_nu_p = numeric(am3)

# Geração das amostras "a posteriori" via com passos MH:

XparMH = function(X, m, V, a, d, x, var_d, up, seed) {
	k = length(up)
	am_mu_p = am_s2_p = am_nu_p = numeric(k)
	cont=0
	set.seed(122019)
	for(i in 1:k) {
		y = rmvnorm(1,mean=c(x[1],x[2],x[3]),sigma=diag(var_d))
		auy = exp(logkpost_re(sam,y[1],exp(y[2]),1/(1+exp(-y[3])),m,V,a,d))
		aux = exp(logkpost_re(sam,x[1],exp(x[2]),1/(1+exp(-x[3])),m,V,a,d))
		acep = min(c(1, auy/aux))
		if(up[i] <= acep) {
			am_mu_p[i] = y[1]
			am_s2_p[i] = exp(y[2])
			am_nu_p[i] = 1/(1+exp(-y[3]))
			x = y; cont = cont + 1
		}
		else {
			am_mu_p[i] = x[1]
			am_s2_p[i] = exp(x[2])
			am_nu_p[i] = 1/(1+exp(-x[3]))
		}
	}
	return(list(XparMH_mu=am_mu_p[(0.2*k + 1):k],
				XparMH_s2=am_s2_p[(0.2*k + 1):k],
				XparMH_nu=am_nu_p[(0.2*k + 1):k], taxa_acep=cont/k))
}
\end{verbatim}

\begin{verbatim}
am1_p = XparMH(X=sam, m=m, V=V, a=a, d=d,
			   x=x, var_d=c(0.0022, 0.0065, 0.0203), up=up1, seed=122019)
am2_p = XparMH(X=sam, m=m, V=V, a=a, d=d,
			   x=x, var_d=c(0.0022, 0.0065, 0.0203), up=up2, seed=122019)
am3_p = XparMH(X=sam, m=m, V=V, a=a, d=d,
			   x=x, var_d=c(0.0022, 0.0065, 0.0203), up=up3, seed=122019)

# Taxa de aceitação do algoritmo em cada cenário de tamanho amostral:

am1_p$taxa_acep; am2_p$taxa_acep; am3_p$taxa_acep

# Média; variância; assimetria; curtose e histogramas das amostras geradas:

st_sam_post(am1_p$XparMH_mu)
hist(am1_p$XparMH_mu, breaks=50, prob=T, main="",
	 xlab=expression(paste(mu)), ylab="")
st_sam_post(am2_p$XparMH_mu)
hist(am2_p$XparMH_mu, breaks=50, prob=T, main="",
	 xlab=expression(paste(mu)), ylab="")
st_sam_post(am3_p$XparMH_mu)
hist(am3_p$XparMH_mu, breaks=50, prob=T, main="",
	 xlab=expression(paste(mu)), ylab="")

st_sam_post(am1_p$XparMH_s2)
hist(am1_p$XparMH_s2, breaks=50, prob=T, main="",
	 xlab=expression(paste(sigma^2)), ylab="")
st_sam_post(am2_p$XparMH_s2)
hist(am2_p$XparMH_s2, breaks=50, prob=T, main="",
	 xlab=expression(paste(sigma^2)), ylab="")
st_sam_post(am3_p$XparMH_s2)
hist(am3_p$XparMH_s2, breaks=50, prob=T, main="",
	 xlab=expression(paste(sigma^2)), ylab="")

st_sam_post(am1_p$XparMH_nu)
hist(am1_p$XparMH_nu, breaks=50, prob=T, main="",
	 xlab=expression(paste(nu)), ylab="")
st_sam_post(am2_p$XparMH_nu)
hist(am2_p$XparMH_nu, breaks=50, prob=T, main="",
	 xlab=expression(paste(nu)), ylab="")
st_sam_post(am3_p$XparMH_nu)
hist(am3_p$XparMH_nu, breaks=50, prob=T, main="",
	 xlab=expression(paste(nu)), ylab="")
\end{verbatim}

\newpage

\begin{verbatim}
# Gráficos de convergência e de autocorrelação das cadeias geradas:

library(coda)

par(mar = c(5,5,3,2))
traceplot(mcmc(am1_p$XparMH_mu), ylab=expression(paste(mu)))
plot(acf(am1_p$XparMH_mu), main="")
traceplot(mcmc(am2_p$XparMH_mu), ylab=expression(paste(mu)))
plot(acf(am2_p$XparMH_mu), main="")
traceplot(mcmc(am3_p$XparMH_mu), ylab=expression(paste(mu)))
plot(acf(am3_p$XparMH_mu), main="")

traceplot(mcmc(am1_p$XparMH_s2), ylab=expression(paste(sigma^2)))
plot(acf(am1_p$XparMH_s2), main="")
traceplot(mcmc(am2_p$XparMH_s2), ylab=expression(paste(sigma^2)))
plot(acf(am2_p$XparMH_s2), main="")
traceplot(mcmc(am3_p$XparMH_s2), ylab=expression(paste(sigma^2)))
plot(acf(am3_p$XparMH_s2), main="")

traceplot(mcmc(am1_p$XparMH_nu), ylab=expression(paste(nu)))
plot(acf(am1_p$XparMH_nu), main="")
traceplot(mcmc(am2_p$XparMH_nu), ylab=expression(paste(nu)))
plot(acf(am2_p$XparMH_nu), main="")
traceplot(mcmc(am3_p$XparMH_nu), ylab=expression(paste(nu)))
plot(acf(am3_p$XparMH_nu), main="")
\end{verbatim}

\end{document}